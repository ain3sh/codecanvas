\section{Introduction}
\label{sec:intro}

LLM-based agents can now navigate repositories, edit files, and drive terminal workflows. Yet they remain brittle when a correct local change has nonlocal consequences. A model may fix a bug in a target function yet violate assumptions held by callers, tests, or downstream scripts that were never inspected. In interactive benchmarks, these regressions appear late (often during verification), forcing expensive backtracking. We refer to this pattern as \textbf{side-effect blindness}: editing without an explicit view of the \emph{dependency surface} (callers, tests, and downstream consumers) that must remain consistent for the change to be correct.

This pattern persists because modern agent loops present code as text snippets, leaving dependency structure implicit. Graph-based tools improve localization by exposing structure and enabling multi-hop traversal~\cite{locagent2025,repograph2024}. Compiler- and verifier-driven systems constrain generation toward valid APIs~\cite{cocogen2024,marin2025}. Yet impact is rarely delivered as a first-class observation \emph{at the moment an edit is chosen}. Instead, agents either infer blast radius from ad hoc searches or discover dependencies reactively via failing tests. Both approaches are fragile under context compaction: once early discoveries drop out of context, agents repeat exploration and lose a consistent rationale for what has already been checked.

We introduce \textbf{CodeCanvas}, an MCP tool that makes impact explicit and persistent. CodeCanvas automatically produces a \emph{codemap} (a bounded, visual dependency slice centered on the current file or symbol), highlighting callers, callees, and test touchpoints. In parallel, it maintains an \emph{Evidence Board} that externalizes reasoning state as claims, evidence, and decisions. Deterministic hooks inject both artifacts during the session (e.g., after file reads), reducing query burden: the agent need not invoke analysis tools explicitly to see what may break~\cite{claudeCodeHooks2025}.

Our goal is to reduce side-effect blindness without changing the underlying agent. We therefore evaluate three observation modalities under a fixed Harbor + Claude Code harness on seven Terminal-Bench~2.0 tasks: (i) text-only interaction, (ii) a graph-based MCP baseline derived from LocAgent, and (iii) CodeCanvas. In this run set, the CodeCanvas condition solves 4/7 tasks (one more than either baseline) and exhibits lower backtracking and lower cost per success; we treat comparisons as descriptive given single trials, mixed-batch provenance, and limited CodeCanvas activation (2/7 tasks).

\paragraph{Contributions.}
\begin{enumerate}
    \item \textbf{CodeCanvas.} An MCP tool that surfaces blast radius as a visual codemap and persists task-local reasoning state via an Evidence Board injected through deterministic hooks.
    \item \textbf{Exploratory fixed-harness comparison of observation modalities.} An evaluation that holds the harness constant while varying the observation channel (text-only vs.\ graph-based vs.\ visual codemaps) on Terminal-Bench~2.0 with ATIF logging~\cite{terminalbench2harbor2025,harborATIF2025}.
    \item \textbf{A preliminary process metric for impact-aware editing.} An \emph{Informed Editing Score} that measures whether edits align with previously observed impact and whether the agent externalizes deliberation; computed only when CodeCanvas state is recorded.
\end{enumerate}
